
import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time

# ================= Setup =================
try:
    import requests_cache
    CACHE_OK = True
except Exception:
    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v4.1)", layout="wide")
st.title("ğŸ“¡ FTTH Geocoding & Matching â€“ v4.1 (Î—ÏÎ¬ÎºÎ»ÎµÎ¹Î¿ & ÎºÎ¿Î½Ï„Î¹Î½Î¬ matches)")

with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Ï„Î¿ Î±Ï†Î®ÏƒÎµÎ¹Ï‚ ÎºÎµÎ½ÏŒ, Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (sec) [Nominatim]", 0.5, 2.0, 1.0, 0.5)
    # ÎÎ•ÎŸ: slider 1â€“20m (default 5m)
    distance_limit = st.slider("ğŸ“ Î‘Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m) Î³Î¹Î± ÎºÎ¿Î½Ï„Î¹Î½Î¬ matches", min_value=1, max_value=20, value=5, step=1)
    city_filter = st.text_input("ğŸ™ Î ÏŒÎ»Î·", "Î—ÏÎ¬ÎºÎ»ÎµÎ¹Î¿ ÎšÏÎ®Ï„Î·Ï‚")

st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î±")
biz_file = st.file_uploader("Excel/CSV Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ (ÏƒÏ„Î®Î»ÎµÏ‚: name, address, city Î® Outscraper: name, site.company_insights.address, site.company_insights.city)", type=["xlsx", "csv"])
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± (Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude)", type=["xlsx", "csv"])
prev_geo_file = st.file_uploader("ğŸ§  Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬) â€“ Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude", type=["xlsx", "csv"])

def load_table(uploaded):
    if uploaded is None: 
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

biz_df = load_table(biz_file) if biz_file else None
ftth_df = load_table(ftth_file) if ftth_file else None
prev_df = load_table(prev_geo_file) if prev_geo_file else None

# ================= Helpers =================
def pick_first_series(df: pd.DataFrame, candidates):
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    cols = {c.lower(): c for c in df.columns}
    if "latitude" not in cols or "longitude" not in cols:
        raise ValueError("Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ FTTH Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude.")
    df = df.rename(columns={cols["latitude"]: "latitude", cols["longitude"]: "longitude"})[["latitude","longitude"]]
    # ğŸ”§ Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· ÎºÏŒÎ¼Î¼Î±Ï„Î¿Ï‚/Ï„ÎµÎ»ÎµÎ¯Î±Ï‚ & Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ float
    df["latitude"]  = pd.to_numeric(df["latitude"].astype(str).str.replace(",", "."), errors="coerce")
    df["longitude"] = pd.to_numeric(df["longitude"].astype(str).str.replace(",", "."), errors="coerce")
    return df.dropna()

# ================= FTTH normalize =================
if ftth_df is not None:
    try:
        ftth_df = normalize_ftth(ftth_df)
    except Exception as e:
        st.error(str(e))
        st.stop()

# ================= Caching & session =================
if CACHE_OK:
    requests_cache.install_cache("geocode_cache", backend="sqlite", expire_after=60*60*24*14)

session = requests.Session()
session.headers.update({"User-Agent": "ftth-app/1.0 (+contact: user)"})

def geocode_nominatim(address, cc="gr", lang="el"):
    params = {"q": address, "format": "json", "limit": 1, "countrycodes": cc, "accept-language": lang}
    r = session.get("https://nominatim.openstreetmap.org/search", params=params, timeout=15)
    r.raise_for_status()
    data = r.json()
    if data:
        return float(data[0]["lat"]), float(data[0]["lon"])
    return None, None

def geocode_google(address, api_key, lang="el"):
    params = {"address": address, "key": api_key, "language": lang}
    r = session.get("https://maps.googleapis.com/maps/api/geocode/json", params=params, timeout=15)
    r.raise_for_status()
    js = r.json()
    if js.get("status") == "OK" and js.get("results"):
        loc = js["results"][0]["geometry"]["location"]
        return float(loc["lat"]), float(loc["lng"])
    return None, None

def geocode_address(address, provider, api_key=None, cc="gr", lang="el", throttle_sec=1.0):
    lat, lon = (None, None)
    if provider.startswith("Google") and api_key:
        lat, lon = geocode_google(address, api_key, lang=lang)
    else:
        lat, lon = geocode_nominatim(address, cc, lang)
        if not getattr(session, "cache_disabled", True):
            time.sleep(throttle_sec)
    if lat is None and "greece" not in address.lower() and "ÎµÎ»Î»Î¬Î´Î±" not in address.lower():
        fallback = f"{address}, Greece"
        if provider.startswith("Google") and api_key:
            lat, lon = geocode_google(fallback, api_key, lang=lang)
        else:
            lat, lon = geocode_nominatim(fallback, cc, lang)
            if not getattr(session, "cache_disabled", True):
                time.sleep(throttle_sec)
    return lat, lon

def _norm(s: str) -> str:
    return (s or "").lower().replace(" ÎºÏÎ·Ï„Î·Ï‚", "").replace(" ÎºÏÎ®Ï„Î·Ï‚", "").strip()

start = st.button("ğŸš€ ÎÎµÎºÎ¯Î½Î± geocoding & matching")

if start and biz_df is not None and ftth_df is not None:
    work = biz_df.copy()
    addr_series = pick_first_series(work, ["address", "site.company_insights.address", "Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·"])
    city_series = pick_first_series(work, ["city", "site.company_insights.city", "Ï€ÏŒÎ»Î·"])

    base_addr = addr_series.astype(str).str.strip()
    fallback_city = city_filter.strip()
    from_input_city = city_series.astype(str).str.strip().replace("", fallback_city)
    work["Address"] = (base_addr + ", " + from_input_city).str.replace(r"\s+", " ", regex=True)

    work = work[work["Address"].str.len() > 3].copy()
    unique_addresses = sorted(work["Address"].dropna().unique().tolist())

    # Resume Î±Ï€ÏŒ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿
    geo_map = {}
    if prev_geo_file is not None and prev_df is not None and {"Address","Latitude","Longitude"}.issubset(prev_df.columns):
        for _, r in prev_df.iterrows():
            geo_map[r["Address"]] = (r["Latitude"], r["Longitude"])

    total = len(unique_addresses)
    progress = st.progress(0, text=f"0 / {total}")
    errs = 0

    for i, addr in enumerate(unique_addresses, start=1):
        if addr in geo_map:
            lat, lon = geo_map[addr]
        else:
            query = f"{addr}, {city_filter}" if city_filter and city_filter.lower() not in addr.lower() else addr
            lat, lon = geocode_address(query, geocoder, api_key=google_key, cc=country, lang=lang, throttle_sec=throttle)
            if lat is not None and lon is not None:
                geo_map[addr] = (lat, lon)
            else:
                errs += 1
        progress.progress(i/total, text=f"{i} / {total} Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î±...")

    geocoded_df = pd.DataFrame([{"Address": a, "Latitude": v[0], "Longitude": v[1]} for a, v in geo_map.items() if v[0] is not None])

    # Join back
    merged = work.merge(geocoded_df, on="Address", how="left")

    # ğŸ”§ ÎšÎ±Î½Î¿Î½Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î´ÎµÎºÎ±Î´Î¹ÎºÏÎ½ ÎºÎ±Î¹ Î¼ÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ float ÏƒÏ„Î¹Ï‚ Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚
    if "Latitude" in merged.columns and "Longitude" in merged.columns:
        merged["Latitude"]  = pd.to_numeric(merged["Latitude"].astype(str).str.replace(",", "."), errors="coerce")
        merged["Longitude"] = pd.to_numeric(merged["Longitude"].astype(str).str.replace(",", "."), errors="coerce")

    # Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ & Î±Î½ÎµÎºÏ„Î¹ÎºÏŒ Ï†Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Ï€ÏŒÎ»Î·Ï‚ (Î´ÎµÎ½ Ï€ÎµÏ„Î¬Î¼Îµ Î±Î½ Î´ÎµÎ½ Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Î±ÎºÏÎ¹Î²ÏÏ‚)
    if city_filter.strip():
        cf = _norm(city_filter)
        merged = merged[merged["Address"].fillna("").apply(lambda x: cf in _norm(x))]

    # ============== Matching Î¼ÏŒÎ½Î¿ ÎºÎ¿Î½Ï„Î¹Î½Î¬ (<= slider) ==============
    ftth_points = ftth_df[["latitude","longitude"]].dropna().to_numpy()
    matches = []
    exact_count = 0

    for _, row in merged.dropna(subset=["Latitude","Longitude"]).iterrows():
        biz_coords = (row["Latitude"], row["Longitude"])
        best_d = None
        best_ft = None
        # Î²ÏÎµÏ‚ Ï„Î¿ ÎºÎ¿Î½Ï„Î¹Î½ÏŒÏ„ÎµÏÎ¿ FTTH
        for ft_lat, ft_lon in ftth_points:
            d = geodesic(biz_coords, (ft_lat, ft_lon)).meters
            if best_d is None or d < best_d:
                best_d = d
                best_ft = (ft_lat, ft_lon)
        # ÎºÏÎ¬Ï„Î± Î¼ÏŒÎ½Î¿ Î±Î½ ÎµÎ¯Î½Î±Î¹ ÎµÎ½Ï„ÏŒÏ‚ Ï„Î¿Ï… slider
        if best_d is not None and best_d <= distance_limit:
            if round(row["Latitude"], 5) == round(best_ft[0], 5) and round(row["Longitude"], 5) == round(best_ft[1], 5):
                exact = True
                exact_count += 1
            else:
                exact = False
            matches.append({
                "name": row.get("name", ""),
                "Address": row["Address"],
                "Latitude": row["Latitude"],
                "Longitude": row["Longitude"],
                "FTTH_lat": best_ft[0],
                "FTTH_lon": best_ft[1],
                "Distance(m)": round(best_d, 2),
                "Exact(5dp)": exact
            })

    result_df = pd.DataFrame(matches).sort_values(["Exact(5dp)", "Distance(m)"], ascending=[False, True]).reset_index(drop=True)
    st.success(f"âœ… ÎšÎ¿Î½Ï„Î¹Î½Î¬ matches (<= {distance_limit} m): {len(result_df)} â€¢ ğŸ¯ Exact (5dp): {exact_count}")
    st.dataframe(result_df, use_container_width=True)

    # -------- Excel export --------
    def to_excel_bytes(df: pd.DataFrame):
        safe = df.copy()
        if safe is None or safe.empty:
            safe = pd.DataFrame([{"info": "no data"}])
        safe.columns = [str(c) for c in safe.columns]
        for c in safe.columns:
            safe[c] = safe[c].apply(lambda x: x if pd.api.types.is_scalar(x) else str(x))
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            safe.to_excel(writer, index=False, sheet_name="Sheet1")
        output.seek(0)
        return output

    col1, col2 = st.columns(2)
    with col1:
        st.download_button("â¬‡ï¸ ÎšÎ¿Î½Ï„Î¹Î½Î¬ matches (Excel)", to_excel_bytes(result_df), file_name=f"ftth_matches_{distance_limit}m.xlsx")
    with col2:
        st.download_button("â¬‡ï¸ Geocoded Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚", to_excel_bytes(geocoded_df), file_name="geocoded_addresses.xlsx")

elif start and (biz_df is None or ftth_df is None):
    st.error("âŒ Î‘Î½Î­Î²Î±ÏƒÎµ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î±: Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ & FTTH ÏƒÎ·Î¼ÎµÎ¯Î±.")
else:
    st.info("ğŸ“„ Î‘Î½Î­Î²Î±ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î±, ÏƒÏ…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎµ Ï€ÏŒÎ»Î· (Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®: Î—ÏÎ¬ÎºÎ»ÎµÎ¹Î¿ ÎšÏÎ®Ï„Î·Ï‚) ÎºÎ±Î¹ Ï€Î¬Ï„Î± Â«ğŸš€ ÎÎµÎºÎ¯Î½Î±Â».")
