# ftth_scraper_nova_streamlit.py
# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time
import re

# ---------- Optional cache ----------
try:
    import requests_cache
    CACHE_OK = True
except Exception:
    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v7)", layout="wide")
st.title("ğŸ“¡ FTTH Geocoding & Matching â€“ v7")

# ================= Sidebar =================
with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Î¼ÎµÎ¯Î½ÎµÎ¹ ÎºÎµÎ½ÏŒ, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (sec) [Nominatim]", 0.5, 2.0, 1.0, 0.5)
    distance_limit = st.number_input("ğŸ“ ÎœÎ­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m)", min_value=1, max_value=500, value=150)

    st.subheader("Î Î·Î³Î® Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½")
    biz_source = st.radio("Î•Ï€Î¹Î»Î¿Î³Î®", ["Upload Excel/CSV", "Î“Î•ÎœÎ— (OpenData API)"], index=0)
    gemi_key = st.text_input("GÎ•ÎœÎ— API Key", type="password") if biz_source == "Î“Î•ÎœÎ— (OpenData API)" else None

    st.caption("âš ï¸ Rate limit Î“Î•ÎœÎ—: 8 req/min (429 Î±Î½ Î¾ÎµÏ€ÎµÏÎ±ÏƒÏ„ÎµÎ¯).")

# ================= Uploads & Inputs =================
st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î±")
biz_file = st.file_uploader("Excel/CSV Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½", type=["xlsx", "csv"]) if biz_source == "Upload Excel/CSV" else None
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± Nova (Excel/CSV) â€“ Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î»/Ï† ÎºÎ±Î¹ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ sheets", type=["xlsx", "csv"])
prev_geo_file = st.file_uploader("ğŸ§  Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬) â€“ Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude", type=["xlsx", "csv"])

# ================= Helpers =================
def load_table(uploaded):
    if uploaded is None:
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

def pick_first_series(df, candidates):
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def _clean_col(s: str) -> str:
    return (
        str(s).lower()
        .replace("(", " ").replace(")", " ")
        .replace("[", " ").replace("]", " ")
        .replace(".", " ").replace(",", " ")
        .replace("Î¬","Î±").replace("Î­","Îµ").replace("Î®","Î·")
        .replace("Î¯","Î¹").replace("ÏŒ","Î¿").replace("Ï","Ï…").replace("Ï","Ï‰")
        .strip()
    )

def _find_col(df: pd.DataFrame, patterns):
    cleaned = {c: _clean_col(c) for c in df.columns}
    for p in patterns:
        for orig, cl in cleaned.items():
            if p in cl:
                return orig
    return None

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    lat_col = _find_col(df, ["latitude", "lat", "Ï€Î»Î±Ï„Î¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Ï€Î»Î±Ï„Î¿Ï‚", "Ï†"])
    lon_col = _find_col(df, ["longitude", "lon", "long", "Î¼Î·ÎºÎ¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Î¼Î·ÎºÎ¿Ï‚", "Î»"])
    if not lat_col or not lon_col:
        raise ValueError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î®Î»ÎµÏ‚ latitude/longitude (Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Î·ÎºÎ±Î½ ÎºÎ±Î¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬: Î Î»Î¬Ï„Î¿Ï‚/ÎœÎ®ÎºÎ¿Ï‚).")
    out = df[[lat_col, lon_col]].rename(columns={lat_col: "latitude", lon_col: "longitude"}).copy()
    out["latitude"]  = pd.to_numeric(out["latitude"].astype(str).str.replace(",", "."), errors="coerce")
    out["longitude"] = pd.to_numeric(out["longitude"].astype(str).str.replace(",", "."), errors="coerce")
    out = out.dropna(subset=["latitude","longitude"])
    return out

def _to_excel_bytes(df: pd.DataFrame):
    output = io.BytesIO()
    if df is None or df.empty:
        df = pd.DataFrame([{"info": "no data"}])
    df.columns = [str(c) for c in df.columns]
    for c in df.columns:
        df[c] = df[c].apply(lambda x: x if pd.api.types.is_scalar(x) else str(x))
    with pd.ExcelWriter(output, engine="openpyxl") as w:
        df.to_excel(w, index=False)
    output.seek(0)
    return output

# ============= GEMI OpenData (ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Swagger) =============
GEMI_BASE = "https://opendata-api.businessportal.gr/api/opendata/v1"
GEMI_HEADER = "api_key"
TIMEOUT = 40

def _hdr(api_key: str):
    return {GEMI_HEADER: api_key, "Accept": "application/json"}

@st.cache_data(ttl=3600, show_spinner=False)
def gemi_metadata(api_key: str):
    """
    Î¦Î­ÏÎ½ÎµÎ¹ Î»Î¯ÏƒÏ„ÎµÏ‚: prefectures, municipalities, companyStatuses, activities.
    Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ dict Î¼Îµ keys: 'prefectures','municipalities','statuses','activities'
    """
    s = requests.Session()
    s.headers.update(_hdr(api_key))
    def _get(ep):
        url = f"{GEMI_BASE}/{ep.lstrip('/')}"
        r = s.get(url, timeout=TIMEOUT)
        r.raise_for_status()
        return r.json()

    data = {}
    data["prefectures"]   = _get("metadata/prefectures")
    data["municipalities"] = _get("metadata/municipalities")
    data["statuses"]      = _get("metadata/companyStatuses")
    # ÎšÎ‘Î” Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Î»Î¬ â€“ Î±Î»Î»Î¬ Ï„Î¿ Î¶Î·Ï„Î®ÏƒÎ±Ï„Îµ: dropdown. Î‘Î½ Â«Î²Î±ÏÏÂ», Î±Î»Î»Î¬Î¶Î¿Ï…Î¼Îµ ÏƒÎµ text input.
    data["activities"]    = _get("metadata/activities")
    return data

def _safe(v, *keys):
    cur = v
    for k in keys:
        if not isinstance(cur, dict):
            return ""
        cur = cur.get(k, "")
    return cur if cur is not None else ""

def companies_to_df(items):
    rows = []
    for it in items:
        # Î¿Î½ÏŒÎ¼Î±Ï„Î±
        name = it.get("coNameEl") or _safe(it, "coTitlesEl") or _safe(it, "coTitlesEn") or ""
        # Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·
        street = it.get("street") or ""
        street_no = it.get("streetNumber") or ""
        address = f"{street} {street_no}".strip()
        # ÎšÎ‘Î” (activities)
        act_list = it.get("activities") or []
        kad_codes = []
        kad_descrs = []
        for a in act_list:
            act = a.get("activity") or {}
            if isinstance(act, dict):
                if act.get("id"):
                    kad_codes.append(str(act.get("id")))
                if act.get("descr"):
                    kad_descrs.append(str(act.get("descr")))
        rows.append({
            "prefecture_id": _safe(it, "prefecture", "id"),
            "prefecture": _safe(it, "prefecture", "descr"),
            "municipality_id": _safe(it, "municipality", "id"),
            "municipality": _safe(it, "municipality", "descr"),
            "city": it.get("city") or "",
            "address": address,
            "zip": it.get("zipCode") or "",
            "email": it.get("email") or "",
            "url": it.get("url") or "",
            "arGemi": it.get("arGemi") or "",
            "afm": it.get("afm") or "",
            "legal_type": _safe(it, "legalType", "descr"),
            "status": _safe(it, "status", "descr"),
            "incorporationDate": it.get("incorporationDate") or "",
            "kad_codes": ";".join(kad_codes),
            "kad_descr": ";".join(kad_descrs),
        })
    df = pd.DataFrame(rows)
    if not df.empty:
        df = df.drop_duplicates().reset_index(drop=True)
    return df

def companies_search(api_key: str, *, name=None, prefectures=None, municipalities=None,
                     statuses=None, activities=None, is_active=None,
                     offset=0, size=200, sort_by="+arGemi"):
    """
    ÎšÎ±Î»ÎµÎ¯ GET /companies ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Swagger.
    - arrays: comma-separated strings (Ï€.Ï‡. '1,2,3')
    """
    s = requests.Session()
    s.headers.update(_hdr(api_key))
    params = {"resultsOffset": offset, "resultsSize": size, "resultsSortBy": sort_by}

    if name and len(name.strip()) >= 3:
        params["name"] = name.strip()

    def _join(x):
        return ",".join([str(i) for i in x]) if x else None

    if prefectures:
        params["prefectures"] = _join(prefectures)
    if municipalities:
        params["municipalities"] = _join(municipalities)
    if statuses:
        params["statuses"] = _join(statuses)
    if activities:
        params["activities"] = _join(activities)
    if is_active is not None:
        params["isActive"] = bool(is_active)

    # ÎºÎ±Î¸Î¬ÏÎ¹ÏƒÎ¼Î± None
    params = {k: v for k, v in params.items() if v not in (None, "", [])}

    url = f"{GEMI_BASE}/companies"
    r = s.get(url, params=params, timeout=TIMEOUT)
    if r.status_code == 429:
        raise RuntimeError("429 Too Many Requests (Ï…Ï€Î­ÏÎ²Î±ÏƒÎ· 8 req/min). Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Ï€Î¬Î»Î¹ Î¼ÎµÏ„Î¬ Î±Ï€ÏŒ Î¼ÎµÏÎ¹ÎºÎ¬ Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î±.")
    r.raise_for_status()
    js = r.json()
    results = js.get("searchResults") or []
    meta = js.get("searchMetadata") or {}
    total = meta.get("totalCount")
    return results, int(total) if isinstance(total, int) or (isinstance(total, str) and total.isdigit()) else None

def companies_export_all(api_key: str, **kw):
    """
    Î Î¿Î»Î»Î±Ï€Î»Î­Ï‚ ÏƒÎµÎ»Î¯Î´ÎµÏ‚ Î¼Îµ ÏƒÎµÎ²Î±ÏƒÎ¼ÏŒ ÏƒÏ„Î¿ 8 req/min:
    - size=200
    - 1Î¿ call â‡’ Ï€Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ totalCount
    - Î­Ï€ÎµÎ¹Ï„Î± loop Î¼Îµ offset += 200 ÎºÎ±Î¹ sleep 8s
    """
    size = kw.pop("size", 200)
    size = max(1, min(200, int(size)))
    out = []

    first, total = companies_search(api_key, size=size, **kw)
    out.extend(first)
    if total is None:
        return out
    if len(out) >= total:
        return out

    offset = size
    while offset < total:
        time.sleep(8.2)  # rate limit guard
        page, _ = companies_search(api_key, offset=offset, size=size, **kw)
        if not page:
            break
        out.extend(page)
        offset += size
    return out

# ============= Î“Î•ÎœÎ— â€“ UI =============
gemi_df = None
if biz_source == "Î“Î•ÎœÎ— (OpenData API)":
    if not gemi_key:
        st.warning("ğŸ”‘ Î’Î¬Î»Îµ GÎ•ÎœÎ— API Key Î³Î¹Î± Î½Î± ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î· Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·.")
    else:
        st.subheader("ğŸ·ï¸ Î“Î•ÎœÎ— â€“ Î•Î¾Î±Î³Ï‰Î³Î® / Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·")
        # Î¦ÏŒÏÏ„Ï‰ÏƒÎ· metadata Î¼Îµ caching
        md_pref, md_muni, md_status, md_act = [], [], [], []
        try:
            meta = gemi_metadata(gemi_key)
            md_pref = meta.get("prefectures") or []
            md_muni = meta.get("municipalities") or []
            md_status = meta.get("statuses") or []
            md_act = meta.get("activities") or []
        except Exception as e:
            st.error(f"Î£Ï†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ metadata: {e}")
            st.info("Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¾Î±Î½Î¬ ÏƒÎµ ~60s (Ï€Î¹Î¸Î±Î½ÏŒ rate limit 429).")

        # Prefectures (ÎÎ¿Î¼Î¿Î¯)
        pref_label_to_id = {}
        if isinstance(md_pref, list):
            for p in md_pref:
                pid = str(p.get("id") or "").strip()
                pdescr = str(p.get("descr") or "").strip()
                if pid and pdescr:
                    pref_label_to_id[pdescr] = pid
        sel_pref = st.multiselect("ÎÎ¿Î¼ÏŒÏ‚", sorted(pref_label_to_id.keys()), default=[])
        sel_pref_ids = [pref_label_to_id[x] for x in sel_pref]

        # Municipalities (Î”Î®Î¼Î¿Î¹) â€“ Ï†Î¹Î»Ï„ÏÎ¬ÏÎ¿Î½Ï„Î±Î¹ Î±Ï€ÏŒ ÎÎ¿Î¼Î¿ÏÏ‚
        muni_label_to_id = {}
        if isinstance(md_muni, list):
            for m in md_muni:
                mid = str(m.get("id") or "").strip()
                mdescr = str(m.get("descr") or "").strip()
                m_pref_id = str(m.get("prefectureId") or "").strip()
                if sel_pref_ids and (m_pref_id not in sel_pref_ids):
                    continue
                if mid and mdescr:
                    muni_label_to_id[f"{mdescr} (#{mid})"] = mid
        sel_muni = st.multiselect("Î”Î®Î¼Î¿Ï‚", sorted(muni_label_to_id.keys()), default=[])
        sel_muni_ids = [muni_label_to_id[x] for x in sel_muni]

        # Statuses
        status_label_to_id = {}
        if isinstance(md_status, list):
            for s in md_status:
                sid = s.get("id")
                sdescr = s.get("descr")
                if sid is not None and sdescr:
                    status_label_to_id[f"{sdescr} (#{sid})"] = sid
        sel_status = st.multiselect("ÎšÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·", sorted(status_label_to_id.keys()), default=[])
        sel_status_ids = [status_label_to_id[x] for x in sel_status]

        # ÎšÎ‘Î” (Activities)
        # Î£Î·Î¼: ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Î»Î¬ â€“ Î±Î»Î»Î¬ Î¼Îµ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÏƒÏ„Î¿ multiselect Î²ÏÎ¯ÏƒÎºÎµÎ¹Ï‚ ÎµÏÎºÎ¿Î»Î±
        act_label_to_id = {}
        if isinstance(md_act, list):
            for a in md_act:
                aid = str(a.get("id") or "").strip()
                adesc = str(a.get("descr") or "").strip()
                if aid:
                    act_label_to_id[f"{aid} â€” {adesc}"] = aid
        sel_acts = st.multiselect("ÎšÎ‘Î” (Î´ÏÎ±ÏƒÏ„Î·ÏÎ¹ÏŒÏ„Î·Ï„ÎµÏ‚)", sorted(act_label_to_id.keys()), default=[])
        sel_act_ids = [act_label_to_id[x] for x in sel_acts]

        # Î›ÎµÎºÏ„Î¹ÎºÏŒ Î¿Î½ÏŒÎ¼Î±Ï„Î¿Ï‚, ÎµÎ½ÎµÏÎ³Î­Ï‚ Î¼ÏŒÎ½Î¿, Î¼Î­Î³ÎµÎ¸Î¿Ï‚ ÏƒÎµÎ»Î¯Î´Î±Ï‚
        name_part = st.text_input("Î•Ï€Ï‰Î½Ï…Î¼Î¯Î± Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ (>=3 Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚ Î³Î¹Î± Ï‡ÏÎ®ÏƒÎ· ÏƒÏ„Î¿ API)", "")
        is_active = st.selectbox("Î•Î½ÎµÏÎ³Î­Ï‚ Î¼ÏŒÎ½Î¿;", ["â€”", "ÎÎ±Î¹", "ÎŒÏ‡Î¹"], index=0)
        is_active_val = None if is_active == "â€”" else (True if is_active == "ÎÎ±Î¹" else False)
        page_size = st.slider("ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ ÏƒÎµÎ»Î¯Î´Î±Ï‚ (Preview/Export)", 10, 200, 200, 10)

        # Client-side Ï†Î¯Î»Ï„ÏÎ± Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±Ï‚ (Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ„Î¿ API)
        c1, c2 = st.columns(2)
        with c1:
            date_from = st.text_input("Î£ÏÏƒÏ„Î±ÏƒÎ· Î±Ï€ÏŒ (YYYY-MM-DD) â€“ client-side", "")
        with c2:
            date_to = st.text_input("Î£ÏÏƒÏ„Î±ÏƒÎ· Î­Ï‰Ï‚ (YYYY-MM-DD) â€“ client-side", "")

        cA, cB = st.columns(2)
        with cA:
            do_preview = st.button("ğŸ” Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ· (<=200)")
        with cB:
            do_export = st.button("â¬‡ï¸ Î•Î¾Î±Î³Ï‰Î³Î® Excel (ÏŒÎ»ÎµÏ‚ Î¿Î¹ ÏƒÎµÎ»Î¯Î´ÎµÏ‚)")

        if do_preview:
            try:
                items, total = companies_search(
                    gemi_key,
                    name=name_part or None,
                    prefectures=sel_pref_ids or None,
                    municipalities=sel_muni_ids or None,
                    statuses=sel_status_ids or None,
                    activities=sel_act_ids or None,
                    is_active=is_active_val,
                    offset=0, size=page_size
                )
                df = companies_to_df(items)
                # client-side date filter
                if not df.empty and (date_from or date_to):
                    dser = pd.to_datetime(df["incorporationDate"], errors="coerce")
                    if date_from:
                        try:
                            df = df[dser >= pd.to_datetime(date_from)]
                        except Exception:
                            pass
                    if date_to:
                        try:
                            df = df[dser <= pd.to_datetime(date_to)]
                        except Exception:
                            pass
                gemi_df = df
                if df.empty:
                    st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚.")
                else:
                    st.success(f"OK: {len(df)} ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ (totalCount: {total if total is not None else 'â€”'})")
                    st.dataframe(df, use_container_width=True)
                    st.download_button("â¬‡ï¸ Excel (Ï€ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·)", _to_excel_bytes(df), file_name="gemi_preview.xlsx")
            except Exception as e:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚: {e}")

        if do_export:
            try:
                with st.spinner("ÎšÎ±Ï„Î­Î²Î±ÏƒÎ¼Î± ÏƒÎµÎ»Î¯Î´Ï‰Î½â€¦ (Ï„Î·ÏÎµÎ¯Ï„Î±Î¹ 8 req/min)"):
                    items = companies_export_all(
                        gemi_key,
                        name=name_part or None,
                        prefectures=sel_pref_ids or None,
                        municipalities=sel_muni_ids or None,
                        statuses=sel_status_ids or None,
                        activities=sel_act_ids or None,
                        is_active=is_active_val,
                        size=page_size
                    )
                df = companies_to_df(items)
                # client-side date filter
                if not df.empty and (date_from or date_to):
                    dser = pd.to_datetime(df["incorporationDate"], errors="coerce")
                    if date_from:
                        try:
                            df = df[dser >= pd.to_datetime(date_from)]
                        except Exception:
                            pass
                    if date_to:
                        try:
                            df = df[dser <= pd.to_datetime(date_to)]
                        except Exception:
                            pass
                if df.empty:
                    st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î³Î¹Î± ÎµÎ¾Î±Î³Ï‰Î³Î®.")
                else:
                    st.success(f"ÎˆÏ„Î¿Î¹Î¼Î¿: {len(df)} ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚.")
                    st.dataframe(df.head(50), use_container_width=True)
                    st.download_button("â¬‡ï¸ Excel â€“ Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ (Î¼Îµ Ï†Î¯Î»Ï„ÏÎ±)", _to_excel_bytes(df), file_name="gemi_filtered.xlsx")
                gemi_df = df
            except Exception as e:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎµÎ¾Î±Î³Ï‰Î³Î®Ï‚: {e}")

# Î‘Î½ ÎµÏ€Î¹Î»ÎµÎ³ÎµÎ¯ Î“Î•ÎœÎ—, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î±Ï…Ï„Î¬ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï‰Ï‚ Ï€Î·Î³Î® ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½
biz_df = None
if biz_source == "Upload Excel/CSV":
    biz_df = load_table(biz_file) if biz_file else None
elif biz_source == "Î“Î•ÎœÎ— (OpenData API)":
    biz_df = gemi_df

# ============= Geocode cache =============
if CACHE_OK:
    requests_cache.install_cache("geocode_cache", backend="sqlite", expire_after=60*60*24*14)

session = requests.Session()
session.headers.update({"User-Agent": "ftth-app/1.0 (+contact: user)"})

def geocode_nominatim(address, cc="gr", lang="el"):
    params = {"q": address, "format": "json", "limit": 1, "countrycodes": cc, "accept-language": lang}
    r = session.get("https://nominatim.openstreetmap.org/search", params=params, timeout=15)
    r.raise_for_status()
    data = r.json()
    if data:
        return float(data[0]["lat"]), float(data[0]["lon"])
    return None, None

def geocode_google(address, api_key, lang="el"):
    params = {"address": address, "key": api_key, "language": lang}
    r = session.get("https://maps.googleapis.com/maps/api/geocode/json", params=params, timeout=15)
    r.raise_for_status()
    js = r.json()
    if js.get("status") == "OK" and js.get("results"):
        loc = js["results"][0]["geometry"]["location"]
        return float(loc["lat"]), float(loc["lng"])
    return None, None

def geocode_address(address, provider, api_key=None, cc="gr", lang="el", throttle_sec=1.0):
    if provider.startswith("Google") and api_key:
        lat, lon = geocode_google(address, api_key, lang=lang)
    else:
        lat, lon = geocode_nominatim(address, cc, lang)
        if not getattr(session, "cache_disabled", True):
            time.sleep(throttle_sec)
    if (lat is None) and ("greece" not in address.lower()) and ("ÎµÎ»Î»Î¬Î´Î±" not in address.lower()):
        fallback = f"{address}, Greece"
        if provider.startswith("Google") and api_key:
            lat, lon = geocode_google(fallback, api_key, lang=lang)
        else:
            lat, lon = geocode_nominatim(fallback, cc, lang)
            if not getattr(session, "cache_disabled", True):
                time.sleep(throttle_sec)
    return lat, lon

# ============= Main: Geocoding & Matching =============
start = st.button("ğŸš€ ÎÎµÎºÎ¯Î½Î± geocoding & matching")

if start and biz_df is not None and ftth_df is not None:
    work = biz_df.copy()

    addr_series = pick_first_series(work, ["address", "site.company_insights.address", "Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·", "Î¿Î´ÏŒÏ‚", "Î´Î¹ÎµÏ…Î¸Ï…Î½ÏƒÎ·"])
    city_series = pick_first_series(work, ["city", "site.company_insights.city", "Ï€ÏŒÎ»Î·"])

    base_addr = addr_series.astype(str).str.strip()
    from_input_city = city_series.astype(str).str.strip()
    work["Address"] = (base_addr + (", " + from_input_city).where(from_input_city.ne(""), "")).str.replace(r"\s+", " ", regex=True)

    work = work[work["Address"].str.len() > 3].copy()

    total = len(work)
    progress = st.progress(0, text=f"0 / {total}")
    errs = 0

    # cache Î±Ï€ÏŒ prev_df (Î±Î½ Î´ÏŒÎ¸Î·ÎºÎµ)
    geo_map = {}
    prev_df = load_table(prev_geo_file) if prev_geo_file is not None else None
    if prev_df is not None and {"Address","Latitude","Longitude"}.issubset({c.title() if c.islower() else c for c in prev_df.columns}):
        cols = {c.lower(): c for c in prev_df.columns}
        p = prev_df.rename(columns={cols.get("address","address"): "Address",
                                    cols.get("latitude","latitude"): "Latitude",
                                    cols.get("longitude","longitude"): "Longitude"})
        p["Latitude"]  = pd.to_numeric(p["Latitude"].astype(str).str.replace(",", "."), errors="coerce")
        p["Longitude"] = pd.to_numeric(p["Longitude"].astype(str).str.replace(",", "."), errors="coerce")
        p = p.dropna(subset=["Latitude","Longitude"])
        for _, r in p.iterrows():
            geo_map[str(r["Address"])] = (float(r["Latitude"]), float(r["Longitude"]))

    work["Latitude"] = pd.NA
    work["Longitude"] = pd.NA

    for i, (idx, row) in enumerate(work.iterrows(), start=1):
        addr = str(row["Address"]).strip()
        if addr in geo_map:
            lat, lon = geo_map[addr]
        else:
            lat, lon = geocode_address(addr, geocoder, api_key=google_key, cc=country, lang=lang, throttle_sec=throttle)
            if lat is not None and lon is not None:
                geo_map[addr] = (lat, lon)
            else:
                errs += 1
                lat, lon = (None, None)

        work.at[idx, "Latitude"]  = lat
        work.at[idx, "Longitude"] = lon
        progress.progress(i/max(1,total), text=f"{i} / {total} Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î±...")

    work["Latitude"]  = pd.to_numeric(work["Latitude"].astype(str).str.replace(",", "."), errors="coerce")
    work["Longitude"] = pd.to_numeric(work["Longitude"].astype(str).str.replace(",", "."), errors="coerce")

    merged = work.copy()

    # Matching
    ftth_points = ftth_df[["latitude","longitude"]].dropna().to_numpy()
    matches = []
    for _, row in merged.dropna(subset=["Latitude","Longitude"]).iterrows():
        try:
            biz_lat = float(str(row["Latitude"]).replace(",", "."))
            biz_lon = float(str(row["Longitude"]).replace(",", "."))
        except Exception:
            continue
        biz_coords = (biz_lat, biz_lon)
        for ft_lat, ft_lon in ftth_points:
            d = geodesic(biz_coords, (float(ft_lat), float(ft_lon))).meters
            if d <= distance_limit:
                matches.append({
                    "name": row.get("name", ""),
                    "Address": row["Address"],
                    "Latitude": biz_lat,
                    "Longitude": biz_lon,
                    "FTTH_lat": float(ft_lat),
                    "FTTH_lon": float(ft_lon),
                    "Distance(m)": round(d, 2)
                })
                break

    result_df = pd.DataFrame(matches)
    if not result_df.empty and "Distance(m)" in result_df.columns:
        result_df = result_df.sort_values("Distance(m)").reset_index(drop=True)

    if result_df.empty:
        st.warning(f"âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¯ÏƒÎµÎ¹Ï‚ ÎµÎ½Ï„ÏŒÏ‚ {distance_limit} m.")
    else:
        st.success(f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(result_df)} ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ ÎµÎ½Ï„ÏŒÏ‚ {distance_limit} m Î±Ï€ÏŒ FTTH.")
        st.dataframe(result_df, use_container_width=True)

    def to_excel_bytes(df: pd.DataFrame):
        safe = df.copy()
        if safe is None or safe.empty:
            safe = pd.DataFrame([{"info": "no data"}])
        safe.columns = [str(c) for c in safe.columns]
        for c in safe.columns:
            safe[c] = safe[c].apply(lambda x: x if pd.api.types.is_scalar(x) else str(x))
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            safe.to_excel(writer, index=False, sheet_name="Sheet1")
        output.seek(0)
        return output

    c1, c2, c3 = st.columns(3)
    with c1:
        st.download_button("â¬‡ï¸ Geocoded Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚ (Î³ÏÎ±Î¼Î¼Î®-Î³ÏÎ±Î¼Î¼Î®)", to_excel_bytes(merged[["Address","Latitude","Longitude"]]), file_name="geocoded_addresses.xlsx")
    with c2:
        st.download_button("â¬‡ï¸ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Matching", to_excel_bytes(result_df), file_name="ftth_matching_results.xlsx")
    with c3:
        st.download_button("â¬‡ï¸ ÎŒÎ»Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± (merged)", to_excel_bytes(merged), file_name="merged_with_geocoded.xlsx")

elif start and (biz_df is None or ftth_df is None):
    st.error("âŒ Î‘Î½Î­Î²Î±ÏƒÎµ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î±: Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ & FTTH ÏƒÎ·Î¼ÎµÎ¯Î±.")
else:
    st.info("ğŸ“„ Î‘Î½Î­Î²Î±ÏƒÎµ FTTH, Î´Î¹Î¬Î»ÎµÎ¾Îµ Ï€Î·Î³Î® ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ (Upload Î® Î“Î•ÎœÎ—), ÎºÎ±Î¹ Ï€Î¬Ï„Î± Â«ğŸš€ ÎÎµÎºÎ¯Î½Î±Â».")
