
import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time

try:
    import requests_cache
    CACHE_OK = True
except Exception:
import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time

try:
    import requests_cache
    CACHE_OK = True
except Exception:
    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v4)", layout="wide")
st.title("ğŸ“¡ FTTH Geocoding & Matching â€“ v4")

with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Ï„Î¿ Î±Ï†Î®ÏƒÎµÎ¹Ï‚ ÎºÎµÎ½ÏŒ, Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (sec) [Nominatim]", 0.5, 2.0, 1.0, 0.5)
    distance_limit = st.number_input("ğŸ“ ÎœÎ­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m)", min_value=1, max_value=1000, value=50)
    city_filter = st.text_input("ğŸ™ Î ÏŒÎ»Î·", "Î—ÏÎ¬ÎºÎ»ÎµÎ¹Î¿ ÎšÏÎ®Ï„Î·Ï‚")

st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î±")
biz_file = st.file_uploader("Excel/CSV Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ (ÏƒÏ„Î®Î»ÎµÏ‚: name, address, city Î® Outscraper: name, site.company_insights.address, site.company_insights.city)", type=["xlsx", "csv"])
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± (Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude)", type=["xlsx", "csv"])
prev_geo_file = st.file_uploader("ğŸ§  Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬) â€“ Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude", type=["xlsx", "csv"])

def load_table(uploaded):
    if uploaded is None: 
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

biz_df = load_table(biz_file) if biz_file else None
ftth_df = load_table(ftth_file) if ftth_file else None
prev_df = load_table(prev_geo_file) if prev_geo_file else None

def pick_first_series(df: pd.DataFrame, candidates):
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    cols = {c.lower(): c for c in df.columns}
    if "latitude" not in cols or "longitude" not in cols:
        raise ValueError("Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ FTTH Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude.")
    return df.rename(columns={cols["latitude"]: "latitude", cols["longitude"]: "longitude"})[["latitude","longitude"]].dropna()

if ftth_df is not None:
    try:
        ftth_df = normalize_ftth(ftth_df)
    except Exception as e:
        st.error(str(e))
        st.stop()

if CACHE_OK:
    requests_cache.install_cache("geocode_cache", backend="sqlite", expire_after=60*60*24*14)

session = requests.Session()
session.headers.update({"User-Agent": "ftth-app/1.0 (+contact: user)"})

def geocode_nominatim(address, cc="gr", lang="el"):
    params = {"q": address, "format": "json", "limit": 1, "countrycodes": cc, "accept-language": lang}
    r = session.get("https://nominatim.openstreetmap.org/search", params=params, timeout=15)
    r.raise_for_status()
    data = r.json()
    if data:
        return float(data[0]["lat"]), float(data[0]["lon"])
    return None, None

def geocode_google(address, api_key, lang="el"):
    params = {"address": address, "key": api_key, "language": lang}
    r = session.get("https://maps.googleapis.com/maps/api/geocode/json", params=params, timeout=15)
    r.raise_for_status()
    js = r.json()
    if js.get("status") == "OK" and js.get("results"):
        loc = js["results"][0]["geometry"]["location"]
        return float(loc["lat"]), float(loc["lng"])
    return None, None

def geocode_address(address, provider, api_key=None, cc="gr", lang="el", throttle_sec=1.0):
    lat, lon = (None, None)
    if provider.startswith("Google") and api_key:
        lat, lon = geocode_google(address, api_key, lang=lang)
    else:
        lat, lon = geocode_nominatim(address, cc, lang)
        if not getattr(session, "cache_disabled", True):
            time.sleep(throttle_sec)
    if lat is None and "greece" not in address.lower() and "ÎµÎ»Î»Î¬Î´Î±" not in address.lower():
        fallback = f"{address}, Greece"
        if provider.startswith("Google") and api_key:
            lat, lon = geocode_google(fallback, api_key, lang=lang)
        else:
            lat, lon = geocode_nominatim(fallback, cc, lang)
            if not getattr(session, "cache_disabled", True):
                time.sleep(throttle_sec)
    return lat, lon

start = st.button("ğŸš€ ÎÎµÎºÎ¯Î½Î± geocoding & matching")

if start and biz_df is not None and ftth_df is not None:
    work = biz_df.copy()
    addr_series = pick_first_series(work, ["address", "site.company_insights.address", "Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·"])
    city_series = pick_first_series(work, ["city", "site.company_insights.city", "Ï€ÏŒÎ»Î·"])

    # Î£Ï…Î½Î¸Î­Ï„Î¿Ï…Î¼Îµ ÎºÎ±Î¸Î±ÏÎ® Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ· + Ï€ÏŒÎ»Î· Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·
    base_addr = addr_series.astype(str).str.strip()
    fallback_city = city_filter.strip()
    from_input_city = city_series.astype(str).str.strip().replace("", fallback_city)
    work["Address"] = (base_addr + ", " + from_input_city).str.replace(r"\s+", " ", regex=True)

    # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· ÎºÎµÎ½ÏÎ½
    work = work[work["Address"].str.len() > 3].copy()

    unique_addresses = sorted(work["Address"].dropna().unique().tolist())

    # Resume Î±Ï€ÏŒ Ï€ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î¿
    geo_map = {}
    if prev_df is not None and {"Address","Latitude","Longitude"}.issubset(prev_df.columns):
        for _, r in prev_df.iterrows():
            geo_map[r["Address"]] = (r["Latitude"], r["Longitude"])

    total = len(unique_addresses)
    progress = st.progress(0, text=f"0 / {total}")
    errs = 0

    for i, addr in enumerate(unique_addresses, start=1):
        if addr in geo_map:
            lat, lon = geo_map[addr]
        else:
            # Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ ÎºÎ±Î¹ Ï„Î·Î½ Ï€ÏŒÎ»Î· Ï„Î¿Ï… sidebar Î³Î¹Î± ÏƒÏ„Î±Î¸ÎµÏÏŒÏ„Î·Ï„Î±
            query = f"{addr}, {city_filter}" if city_filter and city_filter.lower() not in addr.lower() else addr
            lat, lon = geocode_address(query, geocoder, api_key=google_key, cc=country, lang=lang, throttle_sec=throttle)
            if lat is not None and lon is not None:
                geo_map[addr] = (lat, lon)
            else:
                errs += 1
        progress.progress(i/total, text=f"{i} / {total} Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î±...")

    geocoded_df = pd.DataFrame([{"Address": a, "Latitude": v[0], "Longitude": v[1]} for a, v in geo_map.items() if v[0] is not None])

    # Join back
    merged = work.merge(geocoded_df, on="Address", how="left")

    # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± ÏƒÏ„Î·Î½ Ï€ÏŒÎ»Î· Ï„Î¿Ï… sidebar
    merged = merged[merged["Address"].str.contains(city_filter, case=False, na=False)]

    # Matching
    ftth_points = ftth_df[["latitude","longitude"]].dropna().to_numpy()
    matches = []
    for _, row in merged.dropna(subset=["Latitude","Longitude"]).iterrows():
        biz_coords = (row["Latitude"], row["Longitude"])
        for ft_lat, ft_lon in ftth_points:
            d = geodesic(biz_coords, (ft_lat, ft_lon)).meters
            if d <= distance_limit:
                matches.append({
                    "name": row.get("name", ""),
                    "Address": row["Address"],
                    "Latitude": row["Latitude"],
                    "Longitude": row["Longitude"],
                    "FTTH_lat": ft_lat,
                    "FTTH_lon": ft_lon,
                    "Distance(m)": round(d, 2)
                })
                break

    result_df = pd.DataFrame(matches)
    st.success(f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(result_df)} ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ ÏƒÏ„Î·Î½ Ï€ÏŒÎ»Î· '{city_filter}' ÎµÎ½Ï„ÏŒÏ‚ {distance_limit} m Î±Ï€ÏŒ FTTH.")
    st.dataframe(result_df, use_container_width=True)

    def to_excel_bytes(df):
        output = io.BytesIO()
        df.to_excel(output, index=False, engine="openpyxl")
        output.seek(0)
        return output

    col1, col2, col3 = st.columns(3)
    with col1:
        st.download_button("â¬‡ï¸ Geocoded Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚", to_excel_bytes(geocoded_df), file_name="geocoded_addresses.xlsx")
    with col2:
        st.download_button("â¬‡ï¸ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Matching", to_excel_bytes(result_df), file_name="ftth_matching_results.xlsx")
    with col3:
        st.download_button("â¬‡ï¸ ÎŒÎ»Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± (merged)", to_excel_bytes(merged), file_name="merged_with_geocoded.xlsx")

elif start and (biz_df is None or ftth_df is None):
    st.error("âŒ Î‘Î½Î­Î²Î±ÏƒÎµ ÎºÎ±Î¹ Ï„Î± Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î±: Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ & FTTH ÏƒÎ·Î¼ÎµÎ¯Î±.")
else:
    st.info("ğŸ“„ Î‘Î½Î­Î²Î±ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î±, ÏƒÏ…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎµ Ï€ÏŒÎ»Î· ÎºÎ±Î¹ Ï€Î¬Ï„Î± Â«ğŸš€ ÎÎµÎºÎ¯Î½Î±Â».")

    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v3 fast)", layout="wide")
st.title("âš¡ FTTH Geocoding & Matching (Î³ÏÎ®Î³Î¿ÏÎ¿ & Î±Î½Î¸ÎµÎºÏ„Î¹ÎºÏŒ)")

st.markdown("""
- Î¥Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ **Excel/CSV FTTH** (latitude, longitude)
- ÎšÎ¬Î½ÎµÎ¹ **geocoding Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÏ‰Î½** (Î•Î»Î»Î·Î½Î¹ÎºÎ¬ & Greeklish)
- **Caching** Î³Î¹Î± Î½Î± Î¼Î·Î½ Î¾Î±Î½Î±Î¶Î·Ï„Î¬ÎµÎ¹ Ï„Î¹Ï‚ Î¯Î´Î¹ÎµÏ‚ Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚
- **Î‘Ï€Î¿Ï†Ï…Î³Î® Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Ï‰Î½**: Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ ÎºÎ¬Î¸Îµ Î¼Î¿Î½Î±Î´Î¹ÎºÎ® Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ· Î¼Î¯Î± Ï†Î¿ÏÎ¬
- Î•Ï€Î¹Î»Î¿Î³Î® **Nominatim (Î´Ï‰ÏÎµÎ¬Î½)** Î® **Google Geocoding (Î¼Îµ API key)**
""")

# -----------------------------
# Sidebar options
# -----------------------------
with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Î±Ï†Î®ÏƒÎµÎ¹Ï‚ ÎºÎµÎ½ÏŒ, Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î¼ÏŒÎ½Î¿ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language (IETF tag)", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· Î±Î½Î¬ Î±Î¯Ï„Î·Î¼Î± (sec) [Nominatim Î¼ÏŒÎ½Î¿]", 0.5, 2.0, 1.0, 0.5)
    distance_limit = st.number_input("ğŸ“ ÎœÎ­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m)", min_value=1, max_value=1000, value=50, step=1)

# -----------------------------
# File uploads
# -----------------------------
st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î± ÎµÎ¹ÏƒÏŒÎ´Î¿Ï…")
biz_file = st.file_uploader("Excel Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ (ÏƒÏ„Î®Î»ÎµÏ‚: name, address, city **Î®** name, site.company_insights.address, site.company_insights.city)", type=["xlsx", "csv"])
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± (CSV Î® Excel Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude)", type=["csv", "xlsx"])

# Optional resume: upload previously geocoded addresses
prev_geo_file = st.file_uploader("ğŸ§  (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ) Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Î³Î¹Î± resume (xlsx/csv Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude)", type=["xlsx", "csv"])

def load_table(uploaded):
    if uploaded is None: 
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

biz_df = load_table(biz_file) if biz_file else None
ftth_df = load_table(ftth_file) if ftth_file else None
prev_df = load_table(prev_geo_file) if prev_geo_file else None

# -----------------------------
# Normalize helpers & fixes
# -----------------------------
def pick_first_series(df: pd.DataFrame, candidates):
    """Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ ÎœÎ™Î‘ Series Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÏÏ„Î· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· ÏƒÏ„Î®Î»Î· Ï€Î¿Ï… Ï„Î±Î¹ÏÎ¹Î¬Î¶ÎµÎ¹.
    Î‘Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î¼Îµ Î¯Î´Î¹Î¿ ÏŒÎ½Î¿Î¼Î±, Ï€Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î·Î½ Ï€ÏÏÏ„Î· (.iloc[:,0]).
    Î‘Î½ Î´ÎµÎ½ Î²ÏÎµÎ¸ÎµÎ¯, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ ÎºÎµÎ½Î® Series (Î³Î¹Î± Î±ÏƒÏ†Î±Î»Î­Ï‚ concat)."""
    # Î±ÎºÏÎ¹Î²Î­Ï‚ Ï„Î±Î¯ÏÎ¹Î±ÏƒÎ¼Î±
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def normalize_biz_columns(df: pd.DataFrame) -> pd.DataFrame:
    cols_lower = [c.lower() for c in df.columns]
    # Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ outscraper Î¼Î¿ÏÏ†Î®, Î±Ï€Î»ÏÏ‚ ÎºÎ¬Î½Îµ rename ÏƒÎµ address/city
    if "site.company_insights.address" in df.columns and "site.company_insights.city" in df.columns and "name" in df.columns:
        df = df.rename(columns={
            "site.company_insights.address": "address",
            "site.company_insights.city": "city"
        })
        return df
    # Î”Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¬, Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ address/city/name (Î¿Ï€Î¿Î¹Î¿Ï…Î´Î®Ï€Î¿Ï„Îµ casing)
    has_addr = any(c in ["address"] for c in cols_lower)
    has_city = any(c in ["city"] for c in cols_lower)
    has_name = any(c in ["name"] for c in cols_lower)
    if has_addr and has_city and has_name:
        # Î’ÏÎµÏ‚ Ï„Î± Ï€ÏÎ±Î³Î¼Î±Ï„Î¹ÎºÎ¬ Î¿Î½ÏŒÎ¼Î±Ï„Î± (case-insensitive)
        addr_col = [c for c in df.columns if c.lower() == "address"][0]
        city_col = [c for c in df.columns if c.lower() == "city"][0]
        name_col = [c for c in df.columns if c.lower() == "name"][0]
        return df.rename(columns={addr_col: "address", city_col: "city", name_col: "name"})
    raise ValueError("Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚: name, address, city (Î® name, site.company_insights.address, site.company_insights.city).")

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    cols = {c.lower(): c for c in df.columns}
    if "latitude" not in cols or "longitude" not in cols:
        raise ValueError("Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ FTTH Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚: latitude, longitude.")
    return df.rename(columns={cols["latitude"]: "latitude", cols["longitude"]: "longitude"})[["latitude","longitude"]].dropna()

def maybe_load_prev(prev: pd.DataFrame):
    if prev is None: 
        return None
    cols = {c.lower(): c for c in prev.columns}
    needed = {"address","latitude","longitude"}
    if not needed.issubset(set(cols.keys())):
        return None
    return prev.rename(columns={cols["address"]: "Address", cols["latitude"]: "Latitude", cols["longitude"]: "Longitude"})[["Address","Latitude","Longitude"]]

if biz_df is not None:
    try:
        biz_df = normalize_biz_columns(biz_df)
    except Exception as e:
        st.error(str(e))
        st.stop()

if ftth_df is not None:
    try:
        ftth_df = normalize_ftth(ftth_df)
    except Exception as e:
        st.error(str(e))
        st.stop()

prev_df = maybe_load_prev(prev_df)

# -----------------------------
# Geocoding helpers
# -----------------------------
if CACHE_OK:
    requests_cache.install_cache("geocode_cache", backend="sqlite", expire_after=60*60*24*14)  # 14 days

session = requests.Session()
session.headers.update({"User-Agent": "ftth-app/1.0 (+contact: user)"})

def geocode_nominatim(address, cc="gr", lang="el"):
    params = {"q": address, "format": "json", "limit": 1, "countrycodes": cc, "accept-language": lang}
    r = session.get("https://nominatim.openstreetmap.org/search", params=params, timeout=15)
    r.raise_for_status()
    data = r.json()
    if data:
        return float(data[0]["lat"]), float(data[0]["lon"])
    return None, None

def geocode_google(address, api_key, lang="el"):
    params = {"address": address, "key": api_key, "language": lang}
    r = session.get("https://maps.googleapis.com/maps/api/geocode/json", params=params, timeout=15)
    r.raise_for_status()
    js = r.json()
    if js.get("status") == "OK" and js.get("results"):
        loc = js["results"][0]["geometry"]["location"]
        return float(loc["lat"]), float(loc["lng"])
    return None, None

def geocode_address(address, provider, api_key=None, cc="gr", lang="el", throttle_sec=1.0):
    lat, lon = (None, None)
    if provider.startswith("Google") and api_key:
        lat, lon = geocode_google(address, api_key, lang=lang)
    else:
        lat, lon = geocode_nominatim(address, cc, lang)
        # polite throttle Î¼ÏŒÎ½Î¿ ÏŒÏ„Î±Î½ Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ cache hit
        if not getattr(session, "cache_disabled", True):
            time.sleep(throttle_sec)
    if lat is None and "greece" not in address.lower() and "ÎµÎ»Î»Î¬Î´Î±" not in address.lower():
        fallback = f"{address}, Greece"
        if provider.startswith("Google") and api_key:
            lat, lon = geocode_google(fallback, api_key, lang=lang)
        else:
            lat, lon = geocode_nominatim(fallback, cc, lang)
            if not getattr(session, "cache_disabled", True):
                time.sleep(throttle_sec)
    return lat, lon

# -----------------------------
# Main run
# -----------------------------
if biz_df is not None and ftth_df is not None:
    st.subheader("ğŸ”„ Geocoding Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÏ‰Î½")

    work = biz_df.copy()

    # Î§ÏÎ®ÏƒÎ· pick_first_series Î³Î¹Î± Î½Î± Î±Ï€Î¿Ï†ÏÎ³Î¿Ï…Î¼Îµ DataFrame Î±Î½Ï„Î¯ Î³Î¹Î± Series
    addr_series = pick_first_series(work, ["address", "site.company_insights.address", "Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·"])
    city_series = pick_first_series(work, ["city", "site.company_insights.city", "Ï€ÏŒÎ»Î·"])

    work["Address"] = (
        addr_series.astype(str).str.strip()
        + ", "
        + city_series.astype(str).str.strip()
    ).str.replace(r"\s+", " ", regex=True)

    # Remove rows with empty Address
    work = work[work["Address"].str.len() > 3].copy()

    # Deduplicate
    unique_addresses = sorted(work["Address"].dropna().unique().tolist())

    # Start from previous results if given
    geo_map = {}
    if prev_df is not None:
        for _, r in prev_df.iterrows():
            geo_map[r["Address"]] = (r["Latitude"], r["Longitude"])

    total = len(unique_addresses)
    progress = st.progress(0, text=f"0 / {total}")
    errs = 0

    # Î“ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·
    for i, addr in enumerate(unique_addresses, start=1):
        if addr in geo_map:
            lat, lon = geo_map[addr]
        else:
            lat, lon = geocode_address(addr, geocoder, api_key=google_key, cc=country, lang=lang, throttle_sec=throttle)
            if lat is not None and lon is not None:
                geo_map[addr] = (lat, lon)
            else:
                errs += 1
        progress.progress(i/total, text=f"{i} / {total} Î³ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½Î±...")

    # Build geocoded table
    geo_rows = [{"Address": a, "Latitude": v[0], "Longitude": v[1]} for a, v in geo_map.items() if v[0] is not None]
    geocoded_df = pd.DataFrame(geo_rows)

    st.write("ğŸ§  Cache ÎµÎ½ÎµÏÎ³Î®:", CACHE_OK)
    st.write("ğŸ“¦ Î“ÎµÏ‰ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î·Î¼Î­Î½ÎµÏ‚ Î¼Î¿Î½Î±Î´Î¹ÎºÎ­Ï‚ Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚:", len(geocoded_df), "/", total, "(Î±Ï€Î¿Ï„Ï…Ï‡Î¯ÎµÏ‚:", errs, ")")

    # Join back to businesses
    merged = work.merge(geocoded_df, on="Address", how="left")

    # Matching Î¼Îµ FTTH
    st.subheader("ğŸ“¡ Matching Î¼Îµ FTTH")
    ftth_points = ftth_df[["latitude","longitude"]].dropna().to_numpy()

    matches = []
    for _, row in merged.dropna(subset=["Latitude","Longitude"]).iterrows():
        biz_coords = (row["Latitude"], row["Longitude"])
        for ft_lat, ft_lon in ftth_points:
            d = geodesic(biz_coords, (ft_lat, ft_lon)).meters
            if d <= distance_limit:
                matches.append({
                    "name": row.get("name", ""),
                    "Address": row["Address"],
                    "Latitude": row["Latitude"],
                    "Longitude": row["Longitude"],
                    "FTTH_lat": ft_lat,
                    "FTTH_lon": ft_lon,
                    "Distance(m)": round(d, 2)
                })
                break

    result_df = pd.DataFrame(matches)
    st.success(f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(result_df)} ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚ ÎµÎ½Ï„ÏŒÏ‚ {distance_limit} m Î±Ï€ÏŒ FTTH.")
    st.dataframe(result_df, use_container_width=True)

    # Downloads
    col1, col2, col3 = st.columns(3)
    with col1:
        out_geo = io.BytesIO()
        geocoded_df.to_excel(out_geo, index=False)
        out_geo.seek(0)
        st.download_button("â¬‡ï¸ Geocoded Î´Î¹ÎµÏ…Î¸ÏÎ½ÏƒÎµÎ¹Ï‚", out_geo, file_name="geocoded_addresses.xlsx")
    with col2:
        out_res = io.BytesIO()
        result_df.to_excel(out_res, index=False)
        out_res.seek(0)
        st.download_button("â¬‡ï¸ Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Matching", out_res, file_name="ftth_matching_results.xlsx")
    with col3:
        out_all = io.BytesIO()
        merged.to_excel(out_all, index=False)
        out_all.seek(0)
        st.download_button("â¬‡ï¸ ÎŒÎ»Î± Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± (merged)", out_all, file_name="merged_with_geocoded.xlsx")

else:
    st.info("ğŸ“„ Î‘Î½Î­Î²Î±ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î± Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½ & FTTH Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹Ï‚.")
