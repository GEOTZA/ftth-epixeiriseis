# ftth_scraper_nova_streamlit.py
# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time
import re

# ---------- Optional cache ----------
try:
    import requests_cache
    CACHE_OK = True
except Exception:
    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v6)", layout="wide")
st.title("ğŸ“¡ FTTH Geocoding & Matching â€“ v6")

# ========== Sidebar ==========
with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Î¼ÎµÎ¯Î½ÎµÎ¹ ÎºÎµÎ½ÏŒ, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (sec) [Nominatim]", 0.5, 2.0, 1.0, 0.5)
    distance_limit = st.number_input("ğŸ“ ÎœÎ­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m)", min_value=1, max_value=500, value=150)

    st.subheader("Î Î·Î³Î® Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½")
    biz_source = st.radio("Î•Ï€Î¹Î»Î¿Î³Î®", ["Upload Excel/CSV", "Î“Î•ÎœÎ— (OpenData API)"], index=0)

    # --- Sidebar: API (Î“Î•ÎœÎ—) Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ (Ï€Î¬Î½Ï„Î± Î¿ÏÎ±Ï„ÏŒ) ---
    with st.expander("ğŸ”Œ API (Î“Î•ÎœÎ—) Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚", expanded=(biz_source == "Î“Î•ÎœÎ— (OpenData API)")):
        # Î£ÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Swagger:
        # Base URL: https://opendata-api.businessportal.gr/api/opendata/v1
        default_base   = "https://opendata-api.businessportal.gr/api/opendata/v1"
        default_header = "api_key"

        gemi_base  = st.text_input("Base URL", value=st.session_state.get("gemi_base", default_base))
        gemi_hdr   = st.text_input("Header name", value=st.session_state.get("gemi_header", default_header))
        gemi_key   = st.text_input("GEMH API Key", type="password",
                                   value=st.session_state.get("gemi_key", ""))

        # save ÏƒÎµ session_state
        st.session_state.update(gemi_base=gemi_base, gemi_header=gemi_hdr, gemi_key=gemi_key)

        # ÎœÎ¹ÎºÏÏŒ Î´Î¹Î±Î³Î½Ï‰ÏƒÏ„Î¹ÎºÏŒ
        if st.button("ğŸ§ª Test API (Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÎ¹ÎºÎ¬)"):
            try:
                test_urls = [
                    f"{gemi_base.rstrip('/')}/params/regions",
                    f"{gemi_base.rstrip('/')}/params/perifereies",
                    f"{gemi_base.rstrip('/')}/params/peripheries",
                ]
                tried = []
                for u in test_urls:
                    r = requests.get(u, headers={gemi_hdr: gemi_key} if gemi_key else {}, timeout=15)
                    tried.append(u)
                    r.raise_for_status()
                st.success("OK: params endpoints Î±Ï€Î¬Î½Ï„Î·ÏƒÎ±Î½.")
                st.code("\n".join(tried), language="text")
            except Exception as e:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± params: {e}")

# ========== Uploads & Inputs ==========
st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î±")
biz_file = st.file_uploader("Excel/CSV Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½", type=["xlsx", "csv"]) if biz_source == "Upload Excel/CSV" else None
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± Nova (Excel/CSV) â€“ Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î»/Ï† ÎºÎ±Î¹ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ sheets", type=["xlsx", "csv"])
prev_geo_file = st.file_uploader("ğŸ§  Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬) â€“ Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude", type=["xlsx", "csv"])

# ---------- Helpers ----------
def load_table(uploaded):
    if uploaded is None:
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

def pick_first_series(df: pd.DataFrame, candidates):
    """Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î¼Î¯Î± Series Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÏÏ„Î· Ï„Î±Î¹ÏÎ¹Î±ÏƒÏ„Î® ÏƒÏ„Î®Î»Î· (Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´Î¹Ï€Î»Î­Ï‚, Ï€Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î·Î½ 1Î·)."""
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def _clean_col(s: str) -> str:
    return (
        str(s).lower()
        .replace("(", " ").replace(")", " ")
        .replace("[", " ").replace("]", " ")
        .replace(".", " ").replace(",", " ")
        .replace("Î¬","Î±").replace("Î­","Îµ").replace("Î®","Î·")
        .replace("Î¯","Î¹").replace("ÏŒ","Î¿").replace("Ï","Ï…").replace("Ï","Ï‰")
        .strip()
    )

def _find_col(df: pd.DataFrame, patterns: list[str]) -> str | None:
    cleaned = {c: _clean_col(c) for c in df.columns}
    for p in patterns:
        for orig, cl in cleaned.items():
            if p in cl:
                return orig
    return None

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    """Î Î¹Î¬Î½ÎµÎ¹ EN/GR: latitude/longitude Î® Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Ï€Î»Î±Ï„Î¿Ï‚ (Ï†) / Î¼Î·ÎºÎ¿Ï‚ (Î»), ÎºÏŒÎ¼Î¼Î±â†’Ï„ÎµÎ»ÎµÎ¯Î±, float."""
    lat_col = _find_col(df, ["latitude", "lat", "Ï€Î»Î±Ï„Î¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Ï€Î»Î±Ï„Î¿Ï‚", "Ï†"])
    lon_col = _find_col(df, ["longitude", "lon", "long", "Î¼Î·ÎºÎ¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Î¼Î·ÎºÎ¿Ï‚", "Î»"])
    if not lat_col or not lon_col:
        raise ValueError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î®Î»ÎµÏ‚ latitude/longitude (Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Î·ÎºÎ±Î½ ÎºÎ±Î¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬: Î Î»Î¬Ï„Î¿Ï‚/ÎœÎ®ÎºÎ¿Ï‚).")
    out = df[[lat_col, lon_col]].rename(columns={lat_col: "latitude", lon_col: "longitude"}).copy()
    out["latitude"]  = pd.to_numeric(out["latitude"].astype(str).str.replace(",", "."), errors="coerce")
    out["longitude"] = pd.to_numeric(out["longitude"].astype(str).str.replace(",", "."), errors="coerce")
    out = out.dropna(subset=["latitude","longitude"])
    return out

def _first_non_empty(d: dict, keys: list[str], default=""):
    for k in keys:
        v = d.get(k)
        if v is not None and str(v).strip() != "":
            return v
    return default

def _first_key(d: dict, keys: list[str], default=""):
    for k in keys:
        if k in d and d[k]:
            return d[k]
    return default

def _to_excel_bytes(df: pd.DataFrame):
    output = io.BytesIO()
    if df is None or df.empty:
        df = pd.DataFrame([{"info": "no data"}])
    df.columns = [str(c) for c in df.columns]
    for c in df.columns:
        df[c] = df[c].apply(lambda x: x if pd.api.types.is_scalar(x) else str(x))
    with pd.ExcelWriter(output, engine="openpyxl") as w:
        df.to_excel(w, index=False)
    output.seek(0)
    return output

# ---------- FTTH load (Nova) ----------
ftth_df = None
if ftth_file is not None:
    if ftth_file.name.lower().endswith(".xlsx"):
        xls = pd.ExcelFile(ftth_file)
        st.caption("Nova: Î”Î¹Î¬Î»ÎµÎ¾Îµ sheet Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¹Ï‚ ÏƒÏ…Î½Ï„ÎµÏ„Î±Î³Î¼Î­Î½ÎµÏ‚ (Î»/Ï†).")
        sheet_coords = st.selectbox("ğŸ“„ Sheet ÏƒÏ…Î½Ï„ÎµÏ„Î±Î³Î¼Î­Î½Ï‰Î½ (Nova)", xls.sheet_names, index=0)
        df_coords = pd.read_excel(xls, sheet_name=sheet_coords)
        ftth_df = normalize_ftth(df_coords)
    else:
        raw = load_table(ftth_file)
        ftth_df = normalize_ftth(raw)

# ---------- Biz source (file) ----------
biz_df = None
if biz_source == "Upload Excel/CSV":
    biz_df = load_table(biz_file) if biz_file else None

# ---------- GEMI (OpenData API) ----------
# Î£Î©Î£Î¤Î— Î’Î‘Î£Î— API (Swagger base): https://opendata-api.businessportal.gr/api/opendata/v1
GEMI_FALLBACK_BASES = [
    "https://opendata-api.businessportal.gr/api/opendata/v1",  # ÏƒÏ‰ÏƒÏ„ÏŒ
    "https://opendata-api.businessportal.gr/opendata",          # legacy
    "https://opendata-api.businessportal.gr",                   # Ï€Î¿Î»Ï Ï€Î±Î»Î¹ÏŒ
]

TIMEOUT = 40

def _gemi_headers():
    hdr = st.session_state.get("gemi_header", "api_key")
    key = st.session_state.get("gemi_key", "")
    h = {"Accept": "application/json"}
    if key:
        h[hdr] = key
    return h

def _bases():
    first = st.session_state.get("gemi_base", "").strip()
    bases = []
    if first:
        bases.append(first.replace("Î¿pendata", "opendata"))
    for b in GEMI_FALLBACK_BASES:
        if b not in bases:
            bases.append(b)
    return bases

def gemi_params(what: str, *, region_id=None):
    """
    Î Î±Î¯ÏÎ½ÎµÎ¹ Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÎ¹ÎºÎ¬ (regions / regional_units / dimoi / statuses / kad) Î±Ï€ÏŒ /params/*
    """
    endpoints = []
    if what == "regions":
        endpoints = ["params/regions", "params/perifereies", "params/peripheries"]
    elif what in ("regional_units", "perifereiakes_enotites"):
        if region_id is not None:
            endpoints = [
                f"params/regional_units/{region_id}",
                f"params/perifereiakes_enotites/{region_id}",
                f"params/periferiakes_enotites/{region_id}",
                f"params/prefectures/{region_id}",
            ]
        else:
            endpoints = ["params/regional_units", "params/perifereiakes_enotites", "params/periferiakes_enotites", "params/prefectures"]
    elif what in ("dimoi", "municipalities"):
        if region_id is not None:
            endpoints = [f"params/dimoi/{region_id}", f"params/municipalities/{region_id}"]
        else:
            endpoints = ["params/dimoi", "params/municipalities"]
    elif what in ("statuses",):
        endpoints = ["params/statuses", "params/status", "params/company_statuses"]
    elif what in ("kad", "kads"):
        endpoints = ["params/kad", "params/kads", "params/activity_codes", "params/kad_codes", "params/nace"]
    else:
        endpoints = [f"params/{what}"]

    last_err = None
    for base in _bases():
        for ep in endpoints:
            url = f"{base.rstrip('/')}/{ep.lstrip('/')}"
            try:
                r = requests.get(url, headers=_gemi_headers(), timeout=TIMEOUT)
                if r.status_code == 200:
                    return r.json()
                last_err = f"{r.status_code} on {url}"
            except Exception as e:
                last_err = str(e)
                continue
    raise RuntimeError(f"Î“Î•ÎœÎ—: Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ endpoint Î³Î¹Î± '{what}'. Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿: {last_err}")

def gemi_companies_search(*, page=1, per_page=100,
                          name_part=None,
                          region_id=None, regional_unit_id=None, municipality_id=None,
                          status_id=None, kad_list=None,
                          date_from=None, date_to=None):
    """
    Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÎµÏ„Î±Î¹ÏÎµÎ¹ÏÎ½ Î¼Îµ GET /companies (base: /api/opendata/v1).
    """
    variants = [
        {
            "page": page, "per_page": per_page,
            "name": name_part, "name_part": name_part,
            "region_id": region_id, "regional_unit_id": regional_unit_id, "municipality_id": municipality_id,
            "perifereia_id": region_id, "perifereiaki_enotita_id": regional_unit_id, "dimos_id": municipality_id,
            "status_id": status_id,
            "kad": ",".join(kad_list) if kad_list else None,
            "incorporation_date_from": date_from, "incorporation_date_to": date_to,
            "foundation_date_from": date_from, "foundation_date_to": date_to,
            "registration_date_from": date_from, "registration_date_to": date_to,
        },
        {
            "page": page, "page_size": per_page,
            "name": name_part, "name_part": name_part,
            "regionId": region_id, "regionalUnitId": regional_unit_id, "municipalityId": municipality_id,
            "nomosId": regional_unit_id, "dimosId": municipality_id,
            "statusId": status_id,
            "kad": ",".join(kad_list) if kad_list else None,
            "incorporationDateFrom": date_from, "incorporationDateTo": date_to,
            "foundationDateFrom": date_from, "foundationDateTo": date_to,
            "registrationDateFrom": date_from, "registrationDateTo": date_to,
        },
    ]

    last_err = None
    for base in _bases():
        url = f"{base.rstrip('/')}/companies"
        for params in variants:
            q = {k: v for k, v in params.items() if v not in (None, "", [], {})}
            try:
                r = requests.get(url, params=q, headers=_gemi_headers(), timeout=TIMEOUT)
                if r.status_code == 200:
                    return r.json()
                last_err = f"{r.status_code} on {url} keys={list(q.keys())}"
            except requests.RequestException as e:
                last_err = str(e)
                continue
    raise RuntimeError(f"Î“Î•ÎœÎ—: Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î±Ï€Î­Ï„Ï…Ï‡Îµ. Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ ÏƒÏ†Î¬Î»Î¼Î±: {last_err}")

def gemi_companies_all(**kwargs):
    per_page = kwargs.pop("per_page", 200)
    max_pages = kwargs.pop("max_pages", 100)
    sleep_sec = kwargs.pop("sleep_sec", 0.2)

    items = []
    for p in range(1, max_pages +
                   }
                   ]
                
