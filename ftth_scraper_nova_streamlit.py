# ftth_scraper_nova_streamlit.py
import streamlit as st
import pandas as pd
import requests
from geopy.distance import geodesic
import io
import time
import re

# ---------- Optional cache ----------
try:
    import requests_cache
    CACHE_OK = True
except Exception:
    CACHE_OK = False

st.set_page_config(page_title="FTTH Geocoding & Matching (v5)", layout="wide")
st.title("ğŸ“¡ FTTH Geocoding & Matching â€“ v5")

# ========== Sidebar ==========
with st.sidebar:
    st.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")
    geocoder = st.selectbox("Geocoder", ["Nominatim (Î´Ï‰ÏÎµÎ¬Î½)", "Google (API key)"])
    google_key = st.text_input("Google API key", type="password", help="Î‘Î½ Î¼ÎµÎ¯Î½ÎµÎ¹ ÎºÎµÎ½ÏŒ, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Nominatim.")
    country = st.text_input("Country code", "gr")
    lang = st.text_input("Language", "el")
    throttle = st.slider("ÎšÎ±Î¸Ï…ÏƒÏ„Î­ÏÎ·ÏƒÎ· (sec) [Nominatim]", 0.5, 2.0, 1.0, 0.5)
    distance_limit = st.number_input("ğŸ“ ÎœÎ­Î³Î¹ÏƒÏ„Î· Î±Ï€ÏŒÏƒÏ„Î±ÏƒÎ· (m)", min_value=1, max_value=500, value=150)

    st.subheader("Î Î·Î³Î® Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½")
    biz_source = st.radio("Î•Ï€Î¹Î»Î¿Î³Î®", ["Upload Excel/CSV", "Î“Î•ÎœÎ— (OpenData API)"], index=0)
    gemi_key = st.text_input("GÎ•ÎœÎ— API Key", type="password") if biz_source == "Î“Î•ÎœÎ— (OpenData API)" else None

# ========== Uploads & Inputs ==========
st.subheader("ğŸ“¥ Î‘ÏÏ‡ÎµÎ¯Î±")
biz_file = st.file_uploader("Excel/CSV Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÏ‰Î½", type=["xlsx", "csv"]) if biz_source == "Upload Excel/CSV" else None
ftth_file = st.file_uploader("FTTH ÏƒÎ·Î¼ÎµÎ¯Î± Nova (Excel/CSV) â€“ Ï…Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚ Î»/Ï† ÎºÎ±Î¹ Ï€Î¿Î»Î»Î±Ï€Î»Î¬ sheets", type=["xlsx", "csv"])
prev_geo_file = st.file_uploader("ğŸ§  Î ÏÎ¿Î·Î³Î¿ÏÎ¼ÎµÎ½Î± geocoded (Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬) â€“ Excel/CSV Î¼Îµ ÏƒÏ„Î®Î»ÎµÏ‚: Address, Latitude, Longitude", type=["xlsx", "csv"])

# ---------- Helpers ----------
def load_table(uploaded):
    if uploaded is None:
        return None
    name = uploaded.name.lower()
    if name.endswith(".csv"):
        return pd.read_csv(uploaded)
    return pd.read_excel(uploaded)

def pick_first_series(df: pd.DataFrame, candidates):
    """Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î¼Î¯Î± Series Î±Ï€ÏŒ Ï„Î·Î½ Ï€ÏÏÏ„Î· Ï„Î±Î¹ÏÎ¹Î±ÏƒÏ„Î® ÏƒÏ„Î®Î»Î· (Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´Î¹Ï€Î»Î­Ï‚, Ï€Î±Î¯ÏÎ½ÎµÎ¹ Ï„Î·Î½ 1Î·)."""
    for cand in candidates:
        exact = [c for c in df.columns if c.lower() == cand.lower()]
        if exact:
            col = df[exact]
            return col.iloc[:, 0] if isinstance(col, pd.DataFrame) else col
        loose = df.filter(regex=fr"(?i)^{cand}$")
        if loose.shape[1] > 0:
            return loose.iloc[:, 0]
    return pd.Series([""] * len(df), index=df.index, dtype="object")

def _clean_col(s: str) -> str:
    return (
        str(s).lower()
        .replace("(", " ").replace(")", " ")
        .replace("[", " ").replace("]", " ")
        .replace(".", " ").replace(",", " ")
        .replace("Î¬","Î±").replace("Î­","Îµ").replace("Î®","Î·")
        .replace("Î¯","Î¹").replace("ÏŒ","Î¿").replace("Ï","Ï…").replace("Ï","Ï‰")
        .strip()
    )

def _find_col(df: pd.DataFrame, patterns: list[str]) -> str | None:
    cleaned = {c: _clean_col(c) for c in df.columns}
    for p in patterns:
        for orig, cl in cleaned.items():
            if p in cl:
                return orig
    return None

def normalize_ftth(df: pd.DataFrame) -> pd.DataFrame:
    """Î Î¹Î¬Î½ÎµÎ¹ EN/GR: latitude/longitude Î® Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Ï€Î»Î±Ï„Î¿Ï‚ (Ï†) / Î¼Î·ÎºÎ¿Ï‚ (Î»), ÎºÏŒÎ¼Î¼Î±â†’Ï„ÎµÎ»ÎµÎ¯Î±, float."""
    lat_col = _find_col(df, ["latitude", "lat", "Ï€Î»Î±Ï„Î¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Ï€Î»Î±Ï„Î¿Ï‚", "Ï†"])
    lon_col = _find_col(df, ["longitude", "lon", "long", "Î¼Î·ÎºÎ¿Ï‚", "Î³ÎµÏ‰Î³ÏÎ±Ï†Î¹ÎºÎ¿ Î¼Î·ÎºÎ¿Ï‚", "Î»"])
    if not lat_col or not lon_col:
        raise ValueError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î®Î»ÎµÏ‚ latitude/longitude (Î´Î¿ÎºÎ¹Î¼Î¬ÏƒÏ„Î·ÎºÎ±Î½ ÎºÎ±Î¹ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬: Î Î»Î¬Ï„Î¿Ï‚/ÎœÎ®ÎºÎ¿Ï‚).")
    out = df[[lat_col, lon_col]].rename(columns={lat_col: "latitude", lon_col: "longitude"}).copy()
    out["latitude"]  = pd.to_numeric(out["latitude"].astype(str).str.replace(",", "."), errors="coerce")
    out["longitude"] = pd.to_numeric(out["longitude"].astype(str).str.replace(",", "."), errors="coerce")
    out = out.dropna(subset=["latitude","longitude"])
    return out

def _first_key(d: dict, keys: list[str], default=""):
    for k in keys:
        if k in d and d[k]:
            return d[k]
    return default

def _to_excel_bytes(df: pd.DataFrame):
    output = io.BytesIO()
    if df is None or df.empty:
        df = pd.DataFrame([{"info": "no data"}])
    df.columns = [str(c) for c in df.columns]
    for c in df.columns:
        df[c] = df[c].apply(lambda x: x if pd.api.types.is_scalar(x) else str(x))
    with pd.ExcelWriter(output, engine="openpyxl") as w:
        df.to_excel(w, index=False)
    output.seek(0)
    return output

# ---------- GEMI (OpenData API) ----------
GEMI_BASE = "https://opendata-api.businessportal.gr/opendata"
GEMI_HEADER_NAME = "api_key"

def _gemi_headers(api_key: str):
    return {GEMI_HEADER_NAME: api_key, "Accept": "application/json"}

def gemi_params(api_key, what, *, nomos_id=None):
    """
    Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÎ¹ÎºÏÎ½ Î±Ï€ÏŒ Î“Î•ÎœÎ— Î¼Îµ fallbacks ÏƒÎµ ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ¬ slugs.
    """
    candidates = [f"{GEMI_BASE}/params/{what}"]
    if what == "nomoi":
        candidates += [
            f"{GEMI_BASE}/params/prefectures",
            f"{GEMI_BASE}/params/regional_units",
            f"{GEMI_BASE}/params/perifereiakes_enotites",
        ]
    if what == "dimoi":
        if nomos_id is not None:
            candidates += [
                f"{GEMI_BASE}/params/dimoi/{nomos_id}",
                f"{GEMI_BASE}/params/municipalities/{nomos_id}",
                f"{GEMI_BASE}/params/dimoi?nomosId={nomos_id}",
                f"{GEMI_BASE}/params/municipalities?prefectureId={nomos_id}",
            ]
        else:
            candidates += [
                f"{GEMI_BASE}/params/dimoi",
                f"{GEMI_BASE}/params/municipalities",
            ]
    if what == "statuses":
        candidates += [
            f"{GEMI_BASE}/params/status",
            f"{GEMI_BASE}/params/company_statuses",
        ]
    if what in ("kad", "kads"):
        candidates += [
            f"{GEMI_BASE}/params/kad",
            f"{GEMI_BASE}/params/kads",
            f"{GEMI_BASE}/params/activity_codes",
            f"{GEMI_BASE}/params/kad_codes",
            f"{GEMI_BASE}/params/nace",
        ]

    last_err = None
    for url in candidates:
        try:
            r = requests.get(url, headers=_gemi_headers(api_key), timeout=30)
            if r.status_code == 404:
                last_err = f"404 on {url}"
                continue
            r.raise_for_status()
            return r.json()
        except requests.RequestException as e:
            last_err = str(e)
            continue
    raise RuntimeError(f"Î“Î•ÎœÎ—: Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ endpoint Î³Î¹Î± '{what}'. Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ ÏƒÏ†Î¬Î»Î¼Î±: {last_err}")

def gemi_search(api_key, *, nomos_id=None, dimos_id=None, status_id=None,
                name_part=None, kad_list=None, date_from=None, date_to=None,
                page=1, page_size=200):
    """
    Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· ÎµÏ„Î±Î¹ÏÎµÎ¹
